# LLM Development Options & Enhancements

**Projekt:** GIM_local - Local Assistant MVP
**Datum:** 2025-11-18
**Fokus:** Erweiterte Entwicklungsoptionen f√ºr LLM-Integration

---

## Executive Summary

Dieses Dokument pr√§sentiert **10 strategische Erweiterungen**, die dem LLM mehr Entwicklungsoptionen und erweiterte F√§higkeiten bieten. Die Vorschl√§ge sind priorisiert nach **Business Value**, **Implementierungsaufwand** und **Risiko**.

---

## üéØ Priorit√§tsmatrix

| Option | Business Value | Aufwand | Risiko | Priorit√§t |
|--------|----------------|---------|--------|-----------|
| 1. Multi-Modell-Support | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Medium | Low | **HIGH** |
| 2. RAG (Vector DB) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | High | Medium | **HIGH** |
| 3. Code-Execution Sandbox | ‚≠ê‚≠ê‚≠ê‚≠ê | High | High | **MEDIUM** |
| 4. Web-Search Integration | ‚≠ê‚≠ê‚≠ê‚≠ê | Medium | Medium | **HIGH** |
| 5. Plugin-System | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Very High | Medium | **MEDIUM** |
| 6. Streaming Responses | ‚≠ê‚≠ê‚≠ê | Low | Low | **HIGH** |
| 7. Conversation Memory | ‚≠ê‚≠ê‚≠ê‚≠ê | Low | Low | **HIGH** |
| 8. Function Calling Framework | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Medium | Low | **HIGH** |
| 9. Multi-Language Support | ‚≠ê‚≠ê‚≠ê | Medium | Low | **MEDIUM** |
| 10. Self-Improvement Loop | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Very High | High | **LOW** |

---

## 1. Multi-Modell-Support (HIGH Priority)

### Problem
Aktuell ist die Anwendung fest an Ollama gebunden. Nutzer k√∂nnen nicht zwischen verschiedenen LLM-Anbietern wechseln.

### L√∂sung
**Abstrahiere die LLM-Schicht** mit einem einheitlichen Interface, das mehrere Backends unterst√ºtzt:
- Ollama (lokal)
- OpenAI API
- Anthropic Claude API
- Azure OpenAI
- Google Gemini
- Lokale HuggingFace-Modelle

### Implementation

```python
# llm_backends.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class LLMBackend(ABC):
    """Abstract base class for LLM backends."""

    @abstractmethod
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """Send messages and get response."""
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """Check if backend is available."""
        pass


class OllamaBackend(LLMBackend):
    """Existing Ollama implementation."""
    def chat(self, messages, **kwargs):
        # Existing logic
        pass


class OpenAIBackend(LLMBackend):
    """OpenAI API integration."""
    def __init__(self, api_key: str, model: str = "gpt-4o"):
        self.api_key = api_key
        self.model = model

    def chat(self, messages, **kwargs):
        import openai
        client = openai.OpenAI(api_key=self.api_key)
        response = client.chat.completions.create(
            model=self.model,
            messages=messages,
            **kwargs
        )
        return response.choices[0].message.content


class ClaudeBackend(LLMBackend):
    """Anthropic Claude API integration."""
    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-20250219"):
        self.api_key = api_key
        self.model = model

    def chat(self, messages, **kwargs):
        import anthropic
        client = anthropic.Anthropic(api_key=self.api_key)
        # Convert messages format
        system = next((m["content"] for m in messages if m["role"] == "system"), None)
        user_messages = [m for m in messages if m["role"] != "system"]

        response = client.messages.create(
            model=self.model,
            system=system,
            messages=user_messages,
            max_tokens=4096,
            **kwargs
        )
        return response.content[0].text


# Factory pattern
def get_llm_backend(backend_type: str, **config) -> LLMBackend:
    """Factory for creating LLM backends."""
    backends = {
        "ollama": OllamaBackend,
        "openai": OpenAIBackend,
        "claude": ClaudeBackend,
    }

    if backend_type not in backends:
        raise ValueError(f"Unknown backend: {backend_type}")

    return backends[backend_type](**config)
```

### Configuration
```env
# .env
LLM_BACKEND=ollama  # ollama, openai, claude
LLM_MODEL=llama3.1:8b

# For OpenAI
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o

# For Claude
ANTHROPIC_API_KEY=sk-ant-...
CLAUDE_MODEL=claude-3-5-sonnet-20250219
```

### Benefits
- ‚úÖ Flexibilit√§t f√ºr Nutzer
- ‚úÖ A/B-Testing verschiedener Modelle
- ‚úÖ Fallback bei Ausfall eines Services
- ‚úÖ Kostenoptimierung (g√ºnstigere Modelle f√ºr einfache Tasks)

---

## 2. RAG (Retrieval-Augmented Generation) mit Vector Database (HIGH Priority)

### Problem
Das LLM hat nur Zugriff auf hardcodierte System-Prompts und den aktuellen Konversationskontext. Es kann nicht auf gro√üe Wissensdatenbanken zugreifen.

### L√∂sung
**Implementiere RAG-Pipeline** mit Vektor-Datenbank f√ºr semantische Suche:
1. Dokumenten-Indexierung
2. Semantic Search bei Anfragen
3. Relevante Kontexte in LLM-Prompt einf√ºgen

### Implementation

```python
# rag_system.py
from typing import List, Dict, Any
import chromadb
from sentence_transformers import SentenceTransformer
from pathlib import Path

class RAGSystem:
    """Retrieval-Augmented Generation system."""

    def __init__(self, db_path: str = "~/.local/assistant/vector_db"):
        self.db_path = Path(db_path).expanduser()
        self.db_path.mkdir(parents=True, exist_ok=True)

        # Initialize ChromaDB
        self.client = chromadb.PersistentClient(path=str(self.db_path))
        self.collection = self.client.get_or_create_collection(
            name="knowledge_base",
            metadata={"hnsw:space": "cosine"}
        )

        # Embedding model
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')

    def index_document(self, text: str, metadata: Dict[str, Any] = None):
        """Index a document for later retrieval."""
        import hashlib

        doc_id = hashlib.sha256(text.encode()).hexdigest()[:16]
        embedding = self.embedder.encode(text).tolist()

        self.collection.add(
            ids=[doc_id],
            embeddings=[embedding],
            documents=[text],
            metadatas=[metadata or {}]
        )

    def index_directory(self, directory: Path):
        """Index all text files in a directory."""
        for file_path in directory.rglob("*.txt"):
            text = file_path.read_text()
            metadata = {
                "source": str(file_path),
                "filename": file_path.name,
                "modified": file_path.stat().st_mtime
            }
            self.index_document(text, metadata)

    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """Search for relevant documents."""
        query_embedding = self.embedder.encode(query).tolist()

        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )

        docs = []
        for i, doc in enumerate(results['documents'][0]):
            docs.append({
                "content": doc,
                "metadata": results['metadatas'][0][i],
                "distance": results['distances'][0][i]
            })

        return docs

    def augment_prompt(self, user_query: str, system_prompt: str) -> str:
        """Augment system prompt with relevant context."""
        relevant_docs = self.search(user_query, top_k=3)

        if not relevant_docs:
            return system_prompt

        context = "\n\n".join([
            f"**Quelle {i+1}** ({doc['metadata'].get('filename', 'unknown')}):\n{doc['content']}"
            for i, doc in enumerate(relevant_docs)
        ])

        augmented = f"""{system_prompt}

---
**KONTEXTWISSEN** (aus Wissensdatenbank):

{context}

---
Nutze dieses Wissen, wenn es f√ºr die Anfrage relevant ist.
"""
        return augmented
```

### New Tool: index_knowledge

```python
def index_knowledge(path: str):
    """
    Tool: Index documents for RAG.
    Args: path - file or directory to index
    """
    rag = RAGSystem()
    target = safe_path(path, WORKSPACE)

    if target.is_file():
        text = target.read_text()
        rag.index_document(text, {"source": str(target)})
        return {"indexed": str(target), "chunks": 1}

    elif target.is_dir():
        rag.index_directory(target)
        return {"indexed": str(target), "type": "directory"}
```

### Dependencies
```txt
chromadb==0.5.23
sentence-transformers==3.4.1
```

### Benefits
- ‚úÖ LLM hat Zugriff auf gro√üe Wissensdatenbanken
- ‚úÖ Bessere Antworten durch relevanten Kontext
- ‚úÖ Semantische Suche statt Keyword-Matching
- ‚úÖ Offline-f√§hig (keine externe API)

---

## 3. Code-Execution Sandbox (MEDIUM Priority)

### Problem
Das LLM kann nur Dateien lesen/schreiben, aber keinen Code ausf√ºhren (Python, Bash, etc.).

### L√∂sung
**Sichere Sandbox f√ºr Code-Execution** mit Docker oder Pyodide (Python im Browser).

### Implementation (Docker-basiert)

```python
# code_executor.py
import docker
import tempfile
import pathlib
from typing import Dict, Any

class CodeExecutor:
    """Execute code in isolated Docker container."""

    def __init__(self, image: str = "python:3.11-slim"):
        self.client = docker.from_env()
        self.image = image
        # Ensure image is available
        try:
            self.client.images.get(self.image)
        except docker.errors.ImageNotFound:
            print(f"Pulling {self.image}...")
            self.client.images.pull(self.image)

    def execute_python(
        self,
        code: str,
        timeout: int = 10,
        max_output: int = 10000
    ) -> Dict[str, Any]:
        """
        Execute Python code in sandbox.

        Args:
            code: Python code to execute
            timeout: Max execution time in seconds
            max_output: Max output bytes

        Returns:
            Dict with stdout, stderr, exit_code
        """
        with tempfile.TemporaryDirectory() as tmpdir:
            # Write code to temp file
            code_file = pathlib.Path(tmpdir) / "script.py"
            code_file.write_text(code)

            try:
                # Run container
                container = self.client.containers.run(
                    self.image,
                    command=f"python /code/script.py",
                    volumes={tmpdir: {'bind': '/code', 'mode': 'ro'}},
                    network_mode='none',  # No network access
                    mem_limit='256m',      # 256MB RAM limit
                    cpu_quota=50000,       # 50% CPU
                    detach=True,
                    remove=True
                )

                # Wait for completion
                result = container.wait(timeout=timeout)

                # Get logs
                stdout = container.logs(stdout=True, stderr=False).decode()[:max_output]
                stderr = container.logs(stdout=False, stderr=True).decode()[:max_output]

                return {
                    "success": result['StatusCode'] == 0,
                    "stdout": stdout,
                    "stderr": stderr,
                    "exit_code": result['StatusCode']
                }

            except docker.errors.ContainerError as e:
                return {
                    "success": False,
                    "stdout": "",
                    "stderr": str(e),
                    "exit_code": e.exit_status
                }
            except Exception as e:
                return {
                    "success": False,
                    "stdout": "",
                    "stderr": f"Execution error: {e}",
                    "exit_code": -1
                }
```

### New Tool: execute_code

```json
{
  "tool": "execute_code",
  "args": {
    "language": "python",
    "code": "print('Hello from sandbox!')"
  }
}
```

### Safety Considerations
- ‚úÖ No network access
- ‚úÖ Limited CPU/RAM
- ‚úÖ Timeout protection
- ‚úÖ Output size limits
- ‚ö†Ô∏è Still review code before execution

### Benefits
- ‚úÖ LLM kann Code testen
- ‚úÖ Interaktives Debugging
- ‚úÖ Datenanalyse-Workflows
- ‚úÖ Sicher isoliert vom Host-System

---

## 4. Web-Search Integration (HIGH Priority)

### Problem
LLM hat kein Zugriff auf aktuelle Informationen aus dem Internet.

### L√∂sung
**Integriere Web-Search als Tool** (DuckDuckGo, Google Custom Search, SearXNG).

### Implementation (DuckDuckGo - kostenlos)

```python
# web_search.py
from duckduckgo_search import DDGS
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

class WebSearchTool:
    """Web search integration using DuckDuckGo."""

    def __init__(self, max_results: int = 5):
        self.max_results = max_results

    def search(self, query: str) -> List[Dict[str, Any]]:
        """
        Search the web and return results.

        Args:
            query: Search query string

        Returns:
            List of search results with title, url, snippet
        """
        try:
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=self.max_results))

            formatted = []
            for r in results:
                formatted.append({
                    "title": r.get("title", ""),
                    "url": r.get("href", ""),
                    "snippet": r.get("body", "")
                })

            logger.info(f"Web search for '{query}' returned {len(formatted)} results")
            return formatted

        except Exception as e:
            logger.error(f"Web search failed: {e}", exc_info=True)
            return [{"error": str(e)}]

    def search_and_summarize(self, query: str) -> str:
        """Search and format results as text summary."""
        results = self.search(query)

        if not results:
            return "Keine Suchergebnisse gefunden."

        summary = f"**Web-Suche nach '{query}':**\n\n"
        for i, result in enumerate(results, 1):
            if "error" in result:
                summary += f"Fehler: {result['error']}\n"
            else:
                summary += f"{i}. **{result['title']}**\n"
                summary += f"   {result['snippet']}\n"
                summary += f"   URL: {result['url']}\n\n"

        return summary
```

### New Tool: web_search

```json
{
  "tool": "web_search",
  "args": {
    "query": "Latest Python 3.12 features"
  }
}
```

### Dependencies
```txt
duckduckgo-search==7.3.3
```

### Benefits
- ‚úÖ LLM hat Zugriff auf aktuelle Informationen
- ‚úÖ Fact-checking
- ‚úÖ News und Events
- ‚úÖ Keine API-Keys erforderlich (DuckDuckGo)

---

## 5. Plugin-System (MEDIUM Priority)

### Problem
Neue Features erfordern Code-√Ñnderungen am Core. Keine Erweiterbarkeit durch Drittanbieter.

### L√∂sung
**Plugin-Architektur** die dynamisches Laden von Tools erlaubt.

### Implementation

```python
# plugin_system.py
from typing import Dict, Any, Callable
import importlib.util
import inspect
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class PluginManager:
    """Manage dynamically loaded plugins."""

    def __init__(self, plugin_dir: Path):
        self.plugin_dir = plugin_dir
        self.plugins: Dict[str, Callable] = {}
        self.load_all_plugins()

    def load_plugin(self, plugin_file: Path) -> None:
        """Load a single plugin file."""
        try:
            spec = importlib.util.spec_from_file_location(
                plugin_file.stem,
                plugin_file
            )
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)

            # Find all functions decorated with @plugin_tool
            for name, obj in inspect.getmembers(module):
                if hasattr(obj, '_is_plugin_tool'):
                    self.plugins[obj._tool_name] = obj
                    logger.info(f"Loaded plugin tool: {obj._tool_name}")

        except Exception as e:
            logger.error(f"Failed to load plugin {plugin_file}: {e}")

    def load_all_plugins(self) -> None:
        """Load all plugins from plugin directory."""
        if not self.plugin_dir.exists():
            logger.warning(f"Plugin directory does not exist: {self.plugin_dir}")
            return

        for plugin_file in self.plugin_dir.glob("*.py"):
            if plugin_file.name.startswith("_"):
                continue
            self.load_plugin(plugin_file)

    def execute_tool(self, tool_name: str, **kwargs) -> Any:
        """Execute a plugin tool."""
        if tool_name not in self.plugins:
            raise ValueError(f"Unknown plugin tool: {tool_name}")

        return self.plugins[tool_name](**kwargs)

    def get_tool_list(self) -> Dict[str, str]:
        """Get list of available tools with descriptions."""
        tools = {}
        for name, func in self.plugins.items():
            tools[name] = func.__doc__ or "No description"
        return tools


# Decorator for plugin tools
def plugin_tool(name: str):
    """Decorator to mark a function as a plugin tool."""
    def decorator(func):
        func._is_plugin_tool = True
        func._tool_name = name
        return func
    return decorator
```

### Example Plugin

```python
# plugins/weather_plugin.py
from plugin_system import plugin_tool
import requests

@plugin_tool("get_weather")
def get_weather(city: str) -> dict:
    """
    Get current weather for a city.
    Args: city - Name of the city
    """
    # Use free weather API (e.g., wttr.in)
    try:
        response = requests.get(
            f"https://wttr.in/{city}?format=j1",
            timeout=5
        )
        data = response.json()

        current = data['current_condition'][0]
        return {
            "city": city,
            "temperature": current['temp_C'],
            "condition": current['weatherDesc'][0]['value'],
            "humidity": current['humidity'],
            "wind_speed": current['windspeedKmph']
        }
    except Exception as e:
        return {"error": str(e)}
```

### Benefits
- ‚úÖ Einfache Erweiterbarkeit
- ‚úÖ Community-Plugins m√∂glich
- ‚úÖ Keine Core-√Ñnderungen n√∂tig
- ‚úÖ Sandbox f√ºr unsichere Plugins

---

## 6. Streaming Responses (HIGH Priority)

### Problem
User sieht nichts bis die komplette LLM-Antwort fertig ist (kann lange dauern).

### L√∂sung
**Streaming aktivieren** f√ºr progressive Antwort-Anzeige.

### Implementation

```python
# In OllamaClient
def chat_stream(self, messages: List[Dict[str, str]]):
    """Stream chat responses token by token."""
    payload = {"model": self.model, "messages": messages, "stream": True}

    response = requests.post(
        f"{self.base_url}/api/chat",
        json=payload,
        stream=True,
        timeout=120
    )

    for line in response.iter_lines():
        if line:
            data = json.loads(line)
            if 'message' in data:
                yield data['message'].get('content', '')
```

### Usage
```python
print("ü§ñ Assistent: ", end="", flush=True)
for chunk in ollama.chat_stream(messages):
    print(chunk, end="", flush=True)
print()  # Newline at end
```

### Benefits
- ‚úÖ Bessere UX (sofortiges Feedback)
- ‚úÖ Gef√ºhl von Responsiveness
- ‚úÖ Fr√ºher Abbruch bei falscher Richtung m√∂glich

---

## 7. Conversation Memory & Persistence (HIGH Priority)

### Problem
Konversationshistorie geht verloren nach Programmende. Keine Wiederaufnahme.

### L√∂sung
**Persistenz-Layer** f√ºr Konversationen mit SQLite.

### Implementation

```python
# conversation_db.py
import sqlite3
from typing import List, Dict, Any
from datetime import datetime
import json

class ConversationDB:
    """SQLite-based conversation persistence."""

    def __init__(self, db_path: str = "~/.local/assistant/conversations.db"):
        self.db_path = Path(db_path).expanduser()
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = sqlite3.connect(str(self.db_path))
        self._create_tables()

    def _create_tables(self):
        """Create database schema."""
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS conversations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                title TEXT,
                metadata TEXT
            )
        """)

        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS messages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                conversation_id INTEGER,
                role TEXT,
                content TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (conversation_id) REFERENCES conversations(id)
            )
        """)
        self.conn.commit()

    def create_conversation(self, title: str = None) -> int:
        """Create new conversation and return ID."""
        cursor = self.conn.execute(
            "INSERT INTO conversations (title) VALUES (?)",
            (title or f"Conversation {datetime.now().isoformat()}",)
        )
        self.conn.commit()
        return cursor.lastrowid

    def add_message(self, conversation_id: int, role: str, content: str):
        """Add message to conversation."""
        self.conn.execute(
            "INSERT INTO messages (conversation_id, role, content) VALUES (?, ?, ?)",
            (conversation_id, role, content)
        )
        self.conn.commit()

    def get_conversation(self, conversation_id: int) -> List[Dict[str, str]]:
        """Retrieve conversation messages."""
        cursor = self.conn.execute(
            "SELECT role, content FROM messages WHERE conversation_id = ? ORDER BY timestamp",
            (conversation_id,)
        )
        return [{"role": row[0], "content": row[1]} for row in cursor.fetchall()]

    def list_conversations(self, limit: int = 10) -> List[Dict[str, Any]]:
        """List recent conversations."""
        cursor = self.conn.execute(
            """SELECT id, created_at, title,
               (SELECT COUNT(*) FROM messages WHERE conversation_id = conversations.id) as msg_count
               FROM conversations
               ORDER BY created_at DESC
               LIMIT ?""",
            (limit,)
        )
        return [
            {
                "id": row[0],
                "created_at": row[1],
                "title": row[2],
                "message_count": row[3]
            }
            for row in cursor.fetchall()
        ]
```

### New Commands
```
/conversations - List recent conversations
/resume <id>   - Resume conversation by ID
/save          - Save current conversation
```

### Benefits
- ‚úÖ Konversationen √ºberleben Neustart
- ‚úÖ History-Suche
- ‚úÖ Mehrere Sessions parallel
- ‚úÖ Kontext aus fr√ºheren Chats

---

## 8. Function Calling Framework (HIGH Priority)

### Problem
Aktuell ist Tool-Calling ad-hoc mit JSON-Parsing. Kein standardisiertes Format.

### L√∂sung
**OpenAI-kompatibles Function Calling** implementieren.

### Implementation

```python
# function_calling.py
from typing import List, Dict, Any, Callable
from pydantic import BaseModel
import json

class FunctionDefinition(BaseModel):
    """OpenAI-style function definition."""
    name: str
    description: str
    parameters: Dict[str, Any]  # JSON Schema


class FunctionRegistry:
    """Registry of callable functions for LLM."""

    def __init__(self):
        self.functions: Dict[str, Callable] = {}
        self.definitions: List[FunctionDefinition] = []

    def register(
        self,
        func: Callable,
        description: str,
        parameters_schema: Dict[str, Any]
    ):
        """Register a function for LLM calling."""
        self.functions[func.__name__] = func
        self.definitions.append(FunctionDefinition(
            name=func.__name__,
            description=description,
            parameters=parameters_schema
        ))

    def get_tools_spec(self) -> List[Dict[str, Any]]:
        """Get OpenAI-compatible tools specification."""
        return [
            {
                "type": "function",
                "function": {
                    "name": func_def.name,
                    "description": func_def.description,
                    "parameters": func_def.parameters
                }
            }
            for func_def in self.definitions
        ]

    def execute_function(self, name: str, arguments: Dict[str, Any]) -> Any:
        """Execute a registered function."""
        if name not in self.functions:
            raise ValueError(f"Unknown function: {name}")

        return self.functions[name](**arguments)


# Example registration
registry = FunctionRegistry()

registry.register(
    func=list_dir,
    description="List contents of a directory",
    parameters_schema={
        "type": "object",
        "properties": {
            "path": {
                "type": "string",
                "description": "Directory path relative to workspace"
            }
        }
    }
)
```

### Benefits
- ‚úÖ Standardisiert
- ‚úÖ OpenAI-kompatibel
- ‚úÖ Type-safe
- ‚úÖ Selbst-dokumentierend

---

## 9. Multi-Language ASR/TTS (MEDIUM Priority)

### Problem
Aktuell nur deutsche Sprache konfiguriert.

### L√∂sung
**Sprach-Erkennung** und dynamisches Modell-Switching.

### Implementation

```python
# language_detector.py
from faster_whisper import WhisperModel

class MultilingualASR:
    """Multi-language ASR with auto-detection."""

    def __init__(self, model_size: str = "small"):
        self.model = WhisperModel(model_size)

    def transcribe_auto(self, audio_path: str):
        """Transcribe with language auto-detection."""
        segments, info = self.model.transcribe(
            audio_path,
            language=None,  # Auto-detect
            vad_filter=True
        )

        detected_language = info.language
        text = " ".join([seg.text.strip() for seg in segments])

        return {
            "text": text,
            "language": detected_language,
            "confidence": info.language_probability
        }
```

### Benefits
- ‚úÖ International nutzbar
- ‚úÖ Auto-Detection
- ‚úÖ Code-Switching Support

---

## 10. Self-Improvement Loop (LOW Priority, HIGH Risk)

### Problem
LLM kann sich nicht selbst verbessern oder eigenen Code modifizieren.

### L√∂sung
**Reflektions-Loop** der LLM-Performance analysiert und Code vorschl√§gt.

### ‚ö†Ô∏è WARNING: HIGH RISK
- Kann zu instabilen Systemen f√ºhren
- Requires extensive testing
- Security implications

### Concept Only (nicht empfohlen f√ºr MVP)

```python
class SelfImprovementLoop:
    """
    Experimental: Allow LLM to analyze its own performance
    and suggest code improvements.

    ‚ö†Ô∏è USE WITH EXTREME CAUTION ‚ö†Ô∏è
    """

    def __init__(self, llm, code_executor):
        self.llm = llm
        self.executor = code_executor
        self.improvement_log = []

    def analyze_failure(self, task: str, error: str):
        """Ask LLM to analyze why a task failed."""
        analysis_prompt = f"""
        Task failed: {task}
        Error: {error}

        Analyze what went wrong and suggest a code fix.
        Return Python code that would fix this issue.
        """
        # ... (implementation omitted for safety)
```

### Benefits
- ‚úÖ Kontinuierliche Verbesserung
- ‚úÖ Bug-Fixes durch LLM
- ‚ö†Ô∏è ABER: Hohe Risiken!

---

## üöÄ Roadmap Empfehlung

### Phase 1: Quick Wins (1-2 Wochen)
1. ‚úÖ Streaming Responses (einfach, gro√üer UX-Impact)
2. ‚úÖ Conversation Memory (SQLite, einfach)
3. ‚úÖ Multi-Modell-Support (Abstraction Layer)

### Phase 2: Core Features (3-4 Wochen)
4. ‚úÖ Function Calling Framework (refactor Tools)
5. ‚úÖ Web-Search Integration (DuckDuckGo)
6. ‚úÖ RAG System (ChromaDB + Embeddings)

### Phase 3: Advanced (4-6 Wochen)
7. ‚úÖ Plugin-System (Architecture)
8. ‚úÖ Code-Execution Sandbox (Docker)
9. ‚úÖ Multi-Language Support

### Phase 4: Experimental (Future)
10. ‚ö†Ô∏è Self-Improvement (nur wenn alles andere stabil)

---

## üìä Dependencies Overview

```txt
# requirements-extended.txt
# Core (existing)
faster-whisper==1.2.1
sounddevice==0.5.3
numpy==2.3.5
webrtcvad==2.0.10
requests==2.32.5
pydantic==2.12.4
python-dotenv==1.2.1

# Multi-Model Support
openai==1.66.0
anthropic==0.45.1

# RAG System
chromadb==0.5.23
sentence-transformers==3.4.1

# Web Search
duckduckgo-search==7.3.3

# Code Execution
docker==7.1.0

# Plugin System
importlib-metadata==8.5.0

# Conversation DB
sqlalchemy==2.0.36  # Alternative zu raw SQL
```

---

## üéØ Conclusion

Diese 10 Optionen bieten erhebliche Erweiterungen der LLM-F√§higkeiten:

**Top 3 f√ºr sofortigen Impact:**
1. **Multi-Modell-Support** - Flexibilit√§t
2. **RAG System** - Wissensgrundlage
3. **Function Calling** - Standardisierung

**Langfristig transformativ:**
- **Plugin-System** - Community-Ecosystem
- **Code-Execution** - True Agent Capabilities

**Risiko/Reward:**
- Low Risk: Streaming, Memory, Multi-Model
- Medium Risk: RAG, Web-Search, Plugins
- High Risk: Code-Exec, Self-Improvement

---

**Next Steps:**
1. Priorisierung mit Stakeholdern
2. POC f√ºr Top-3 Features
3. Architektur-Review
4. Iterative Implementation

**Fragen?** ‚Üí Siehe buglog.md f√ºr weitere Details
